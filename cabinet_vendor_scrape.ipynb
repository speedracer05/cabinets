{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4cfb2e7-8920-4f38-8779-c6b693dbb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da714ab2-7ec0-404d-b10e-c060cd75fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Configuration Flags\n",
    "# -------------------------\n",
    "TEST_MODE = True        # Set to False to run through the entire page\n",
    "ROWS_TO_SCRAPE = 5      # Only used when TEST_MODE is True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c1bb63-7d58-44df-ae9b-b7fc5b627bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Logging configuration\n",
    "# -------------------------\n",
    "logging.basicConfig(\n",
    "    filename='scraping_errors.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Set up Selenium WebDriver using webdriver_manager\n",
    "# -------------------------\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # run in headless mode; comment out if debugging\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "wait = WebDriverWait(driver, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34796bc0-7802-4e5d-8881-56be37dc60eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Data saved to C:\\Users\\jchan\\csi360_fire_police\\cabinet_vendors_list\\cabinets\\resources\\output\\vendor_list.csv.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------\n",
    "# Main URL Scraper Configuration \n",
    "# ------------------------------\n",
    "base_exhibitors_url = \"https://kbis2025.smallworldlabs.com/exhibitors\"\n",
    "output_csv = r\"C:\\Users\\jchan\\csi360_fire_police\\cabinet_vendors_list\\cabinets\\resources\\output\\vendor_list.csv\"\n",
    "exhibitor_data = []\n",
    "\n",
    "try:\n",
    "    # 1. Load the exhibitors page and wait for the table's tbody element\n",
    "    logging.info(f\"Loading exhibitors page: {base_exhibitors_url}\")\n",
    "    driver.get(base_exhibitors_url)\n",
    "    # Use the full XPath to the table's tbody\n",
    "    table_body_xpath = \"/html/body/div[2]/div[3]/div[2]/div/div/div/section/div[5]/div[2]/div[1]/table/tbody\"\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, table_body_xpath)))\n",
    "    table_body = driver.find_element(By.XPATH, table_body_xpath)\n",
    "    \n",
    "    rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "    logging.info(f\"Found {len(rows)} rows in the exhibitors table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred while loading the exhibitors page: {e}\")\n",
    "\n",
    "# 2. Build a list of exhibitor URLs from the table\n",
    "exhibitor_urls = []\n",
    "for i, row in enumerate(rows):\n",
    "    if TEST_MODE and i >= ROWS_TO_SCRAPE:\n",
    "        logging.info(\"Test mode enabled: Stopping after processing first few rows.\")\n",
    "        break\n",
    "    try:\n",
    "        # Use relative XPath to get the link element from the row\n",
    "        link_elem = row.find_element(By.XPATH, \".//td[2]/span/a\")\n",
    "        url = link_elem.get_attribute(\"href\")\n",
    "        exhibitor_urls.append(url)\n",
    "    except NoSuchElementException as e:\n",
    "        logging.error(f\"Could not find exhibitor link in row {i+1}: {e}\")\n",
    "\n",
    "# 3. Iterate over each exhibitor URL to extract the website only\n",
    "for i, url in enumerate(exhibitor_urls):\n",
    "    logging.info(f\"Processing exhibitor {i+1} URL: {url}\")\n",
    "    driver.get(url)\n",
    "    # Wait for the upper section of the page to load\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, \"/html/body/div[2]/div[3]/div[1]\")))\n",
    "    \n",
    "    try:\n",
    "        # Extract the website using the provided absolute XPath\n",
    "        website_xpath = \"/html/body/div[2]/div[3]/div[1]/div/div/div/div/div/div/div[3]/div/div[1]/div[3]/div[2]\"\n",
    "        website = driver.find_element(By.XPATH, website_xpath).text.strip()\n",
    "        exhibitor_data.append({\"Website\": website})\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting website for exhibitor at {url}: {e}\")\n",
    "    \n",
    "    # Wait 2 seconds between processing exhibitors\n",
    "    time.sleep(2)\n",
    "\n",
    "# Ensure the driver is closed even if an error occurred\n",
    "driver.quit()\n",
    "\n",
    "# -------------------------\n",
    "# Ensure the output directory exists\n",
    "# -------------------------\n",
    "output_dir = os.path.dirname(output_csv)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Save extracted data to CSV\n",
    "# -------------------------\n",
    "csv_headers = [\"Website\"]\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "    for item in exhibitor_data:\n",
    "        writer.writerow(item)\n",
    "\n",
    "logging.info(f\"Scraping completed. Data saved to {output_csv}.\")\n",
    "print(f\"Scraping completed. Data saved to {output_csv}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8b295-f026-4aa2-a536-2eea93aec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Main Scraper Configuration\n",
    "# -------------------------\n",
    "base_exhibitors_url = \"https://kbis2025.smallworldlabs.com/exhibitors\"\n",
    "output_csv = r\"C:\\Users\\jchan\\csi360_fire_police\\cabinet_vendors_list\\cabinets\\resources\\output\\vendor_list.csv\"\n",
    "exhibitor_data = []\n",
    "\n",
    "try:\n",
    "    # 1. Load the exhibitors page and wait for the table's tbody element\n",
    "    logging.info(f\"Loading exhibitors page: {base_exhibitors_url}\")\n",
    "    driver.get(base_exhibitors_url)\n",
    "    # Use the full XPath to the table's tbody\n",
    "    table_body_xpath = \"/html/body/div[2]/div[3]/div[2]/div/div/div/section/div[5]/div[2]/div[1]/table/tbody\"\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, table_body_xpath)))\n",
    "    table_body = driver.find_element(By.XPATH, table_body_xpath)\n",
    "    \n",
    "    rows = table_body.find_elements(By.TAG_NAME, \"tr\")\n",
    "    logging.info(f\"Found {len(rows)} rows in the exhibitors table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred: {e}\")\n",
    "\n",
    "   # 2. Build a list of exhibitor URLs from the table\n",
    "    exhibitor_urls = []\n",
    "    for i, row in enumerate(rows):\n",
    "        if TEST_MODE and i >= ROWS_TO_SCRAPE:\n",
    "            logging.info(\"Test mode enabled: Stopping after processing first few rows.\")\n",
    "            break\n",
    "        try:\n",
    "            # Use relative XPath to get the link element from the row\n",
    "            link_elem = row.find_element(By.XPATH, \".//td[2]/span/a\")\n",
    "            url = link_elem.get_attribute(\"href\")\n",
    "            exhibitor_urls.append(url)\n",
    "        except NoSuchElementException as e:\n",
    "            logging.error(f\"Could not find exhibitor link in row {i+1}: {e}\")\n",
    "\n",
    "    # 3. Iterate over each exhibitor URL to extract data\n",
    "    for i, url in enumerate(exhibitor_urls):\n",
    "        logging.info(f\"Processing exhibitor {i+1} URL: {url}\")\n",
    "        driver.get(url)\n",
    "        # Wait for the new page to load; here we wait for an element in the upper section of the page\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"/html/body/div[2]/div[3]/div[1]\")))\n",
    "        \n",
    "        try:\n",
    "            # Extract data using the provided absolute XPaths:\n",
    "            company_name_xpath = \"/html/body/div[2]/div[3]/div[1]/div/div/div/div/div/div/div[3]/div/div[1]/div[1]/div[2]\"\n",
    "            description_xpath  = \"/html/body/div[2]/div[3]/div[1]/div/div/div/div/div/div/div[3]/div/div[1]/div[2]/div[2]\"\n",
    "            website_xpath      = \"/html/body/div[2]/div[3]/div[1]/div/div/div/div/div/div/div[3]/div/div[1]/div[3]/div[2]\"\n",
    "            categories_xpath   = \"/html/body/div[2]/div[3]/div[1]/div/div/div/div/div/div/div[3]/div/div[1]/div[4]/div[2]/div\"\n",
    "            \n",
    "            company_name = driver.find_element(By.XPATH, company_name_xpath).text.strip()\n",
    "            description  = driver.find_element(By.XPATH, description_xpath).text.strip()\n",
    "            website      = driver.find_element(By.XPATH, website_xpath).text.strip()\n",
    "            categories   = driver.find_element(By.XPATH, categories_xpath).text.strip()\n",
    "            \n",
    "            exhibitor_data.append({\n",
    "                \"Company Name\": company_name,\n",
    "                \"Description\": description,\n",
    "                \"Website\": website,\n",
    "                \"Categories\": categories\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting data for exhibitor at {url}: {e}\")\n",
    "\n",
    "    # 4. Wait briefly if needed (2 seconds)\n",
    "        time.sleep(2)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during scraping: {str(e)}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba183107-5d38-4e1b-8617-fbf1f7573a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Ensure the output directory exists\n",
    "# -------------------------\n",
    "output_dir = os.path.dirname(output_csv)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Save extracted data to CSV\n",
    "# -------------------------\n",
    "csv_headers = [\"Company Name\", \"Description\", \"Website\", \"Categories\"]\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "    for item in exhibitor_data:\n",
    "        writer.writerow(item)\n",
    "\n",
    "logging.info(f\"Scraping completed. Data saved to {output_csv}.\")\n",
    "print(f\"Scraping completed. Data saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa274410-0c6e-438c-876f-cb044fbba920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
