{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfb2e7-8920-4f38-8779-c6b693dbb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# -------------------------\n",
    "# Logging configuration\n",
    "# -------------------------\n",
    "logging.basicConfig(\n",
    "    filename='scraping_errors.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Set up Selenium WebDriver using webdriver_manager\n",
    "# -------------------------\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # run in headless mode; comment out if debugging\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "wait = WebDriverWait(driver, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d7c951-2eb1-41b2-88c8-50266e7e5d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------\n",
    "# # Helper Function: Safe extraction for Selenium elements using XPath\n",
    "# # -------------------------\n",
    "# def safe_extract(driver, xpath, default=\"0\"):\n",
    "#     try:\n",
    "#         element = driver.find_element(By.XPATH, xpath)\n",
    "#         text = element.text.strip()\n",
    "#         return text if text else default\n",
    "#     except NoSuchElementException:\n",
    "#         logging.error(f\"Element not found for XPath: {xpath}\")\n",
    "#         return default\n",
    "\n",
    "# def safe_extract_attribute(driver, xpath, attribute, default=\"0\"):\n",
    "#     try:\n",
    "#         element = driver.find_element(By.XPATH, xpath)\n",
    "#         attr_val = element.get_attribute(attribute)\n",
    "#         return attr_val.strip() if attr_val else default\n",
    "#     except NoSuchElementException:\n",
    "#         logging.error(f\"Element not found for XPath (attribute {attribute}): {xpath}\")\n",
    "#         return default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a8b295-f026-4aa2-a536-2eea93aec26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Main Scraper Configuration\n",
    "# -------------------------\n",
    "base_exhibitors_url = \"https://kbis2025.smallworldlabs.com/exhibitors\"\n",
    "output_csv = r\"C:\\Users\\jchan\\csi360_fire_police\\cabinet_vendors_list\\cabinets\\resources\\output\\vendor_list.csv\"\n",
    "exhibitor_data = []\n",
    "\n",
    "try:\n",
    "    # 1. Load the exhibitors page and wait for dynamic content\n",
    "    logging.info(f\"Loading exhibitors page: {base_exhibitors_url}\")\n",
    "    driver.get(base_exhibitors_url)\n",
    "    wait.until(EC.presence_of_element_located((By.XPATH, \"//table[contains(@class, 'table-sm')]\")))\n",
    "    \n",
    "   \n",
    "    # 2. Locate the table and its tbody element\n",
    "    table = driver.find_element(By.XPATH, \"//table[contains(@class, 'table-sm')]\")\n",
    "    tbody = table.find_element(By.TAG_NAME, \"tbody\")\n",
    "    rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "    logging.info(f\"Found {len(rows)} rows in the exhibitors table.\")\n",
    "    \n",
    "    # 3. Iterate through each row in the table and extract data\n",
    "    # Assuming the table columns are in the following order:\n",
    "    # Company Name, Description, Founded, Website, Categories, Key Words\n",
    "    for row in rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        if len(cells) >= 6:\n",
    "            data = {\n",
    "                \"Company Name\": cells[0].text.strip() or \"0\",\n",
    "                \"Description\": cells[1].text.strip() or \"0\",\n",
    "                \"Founded\": cells[2].text.strip() or \"0\",\n",
    "                \"Website\": cells[3].text.strip() or \"0\",\n",
    "                \"Categories\": cells[4].text.strip() or \"0\",\n",
    "                \"Key Words\": cells[5].text.strip() or \"0\"\n",
    "            }\n",
    "            exhibitor_data.append(data)\n",
    "        else:\n",
    "            logging.error(f\"Row with unexpected number of cells: {len(cells)}\")\n",
    "    \n",
    "    # 4. Wait briefly if needed (2 seconds)\n",
    "        time.sleep(2)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occurred during scraping: {str(e)}\")\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba183107-5d38-4e1b-8617-fbf1f7573a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Ensure the output directory exists\n",
    "# -------------------------\n",
    "output_dir = os.path.dirname(output_csv)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Save extracted data to CSV\n",
    "# -------------------------\n",
    "csv_headers = [\"Company Name\", \"Description\", \"Founded\", \"Website\", \"Categories\", \"Key Words\"]\n",
    "with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "    writer.writeheader()\n",
    "    for item in exhibitor_data:\n",
    "        writer.writerow(item)\n",
    "\n",
    "logging.info(f\"Scraping completed. Data saved to {output_csv}.\")\n",
    "print(f\"Scraping completed. Data saved to {output_csv}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
